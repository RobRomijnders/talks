# Brief description
Recent advancements in Natural Language Processing (NLP) use deep learning to improve performance. In September 2016, Google [announced](https://tinyurl.com/z2bk3z7) that Google Translate will shift from phrase-based to neural machine translation. Other fields of NLP are making a similar shift. This talk motivates and explains these algorithms and discusses implementations.

# Detailed abstract
Recent advancements in Natural Language Processing (NLP) use deep learning algorithms to improve performance. Google Translate [shifts to neural machine translation](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html), Baidu speech genetarion [uses neural nets ](http://research.baidu.com/deep-voice-production-quality-text-speech-system-constructed-entirely-deep-neural-networks/) and question answering [too](https://www.technologyreview.com/s/538821/computers-are-getting-a-dose-of-common-sense/).  These neural networks share common architectures. They exploit recurrent computation to traverse the input and output. This talk will motivate the recurrent neural networks and discuss architectures. In the second half we discuss extensions such as attention mechanisms.


As always, I am curious to any comments and questions. Reach me at romijndersrob@gmail.com